# 'A', Automatic means that the kernel handles this by itself at startup calculation or adjustment during runtime and there is practically no need to adjust the parameter
# What values does the kernel automatically calculate on boot/adjusts dynamically and does NOT use pre-defined values?
# They should be decided by the kernel based on system parameters, NOT be pre-defined constants declared by the developers. I could not find reliable documentation providing the specific information.
# Lines marked as 'A' are commented out to let the kernel decide. The written options in it are for some fallback, having no effect.

# Values for the "default" on a scale will be edited rarely. Only limits for min and max will be usually modified.

# Sources considered in the creation:
# https://n1kobg.blogspot.com/p/blog-page_23.html?m=1

# Filesystem
fs.file-max = 9223372036854775806 # Allows more files to stay open
fs.aio-max-nr = 2147483647
fs.inotify.max_user_instances=2147483647
fs.inotify.max_user_watches=2147483647

# Kernel main parameters
kernel.oops_limit = 0
kernel.panic_on_oops = 0
kernel.panic_on_stackoverflow = 0
kernel.sched_migration_cost_ns = 5000000 # Keeps threads on the same core for longer
kernel.sched_autogroup_enabled = 0 # meant to improve fairness tasks, but hurts performance for HPC and gaming. https://forum.proxmox.com/threads/increase-performance-with-sched_autogroup_enabled-0.41729/ # http://www.youtube.com/watch?v=prxInRdaNfc # 1 for overall desktop efficiency, 0 for datacenter HPC and LLM inferencing
kernel.sched_schedstats = 0 # Less debugging overhead
kernel.sysrq = 1
kernel.split_lock_mitigate=0
# No watchdogs - less debugging overhead
kernel.nmi_watchdog=0
kernel.soft_watchdog=0
kernel.watchdog=0
kernel.pid_max = 4194304 # Large number of allocatable processes

# net - Networking
net.core.bpf_jit_enable = 1 # converts BPF bytecode into native machine code for faster execution. Set to 2 for logs
net.core.bpf_jit_harden = 0 # Hardening costs performance → disable on trusted desktop
net.core.dev_weight = 1024 # A higher dev_weight can allow the kernel to process more packets in a single pass, potentially reducing CPU overhead and improving throughput for high-bandwidth applications. But 65536 can starve userspace, cause GUI stuttering
net.core.dev_weight_tx_bias = 1 # Setting this higher than RX bias can cause traffic starvation
net.core.dev_weight_rx_bias = 1 # Setting this higher than TX bias can cause traffic starvation
net.core.netdev_budget = 2048
net.core.skb_defer_max = 256
net.core.netdev_max_backlog = 32768 # 2147483647 OR 1024, not used
net.core.optmem_max = 262144 # 2147483647, not used
#net.core.rmem_default = 262144 # A
#net.core.rmem_max = 2147483647 # A
net.core.somaxconn = 2147483647
#net.core.wmem_default = 262144 # A
#net.core.wmem_max = 2147483647 # A
net.ipv4.autoconf = 1 # enables automatic configuration of IPv4 interfaces
net.ipv4.addr_gen_mode = 0
net.ipv4.tcp_shrink_window = 1 
net.ipv4.tcp_window_scaling = 1 # allows TCP window sizes to exceed the original 65,535-byte limit
net.ipv4.tcp_max_syn_backlog = 2147483647
net.ipv4.tcp_syncookies = 1 # Protects against attacks
net.ipv4.tcp_tw_reuse = 1 # Allow fast socket reuse
net.ipv4.accept_ra = 2
net.ipv4.tcp_congestion_control = bbr # Bazzite doesn't support bbr2 yet
net.core.default_qdisc = fq_codel
net.ipv4.tcp_autocorking = 0 # automatic TCP corking. Can cause greater latency, good for efficient data throughput. Disable for gaming
net.ipv4.ip_dynaddr = 1 # dynamically change the source IP address of packets and sockets during retransmissions
net.ipv4.tcp_fastopen = 3 # allows clients to send data in the initial SYN packet, potentially reducing latency
net.ipv4.tcp_low_latency = 1 # Lower latency prioritized over maximum throughput
net.ipv4.tcp_mtu_probing = 2 # Probe for MTU regardless of black holes
net.ipv4.tcp_moderate_rcvbuf = 1 # dynamically picks an ideal receive buffer between your min and max based on measured BDP (bandwidth-delay product) and application read rate
#net.ipv4.tcp_max_tw_buckets = 2000000 # A
# Automatic prevents OOM errors and excess starvation. Forcing 2147483647 is almost always worse.
# A single idle TCP socket can still grow huge if some buggy app or peer triggers the auto-tuning logic\
# Setting minimums to 1 makes the system less stable under memory pressure, not more. It prevents the TCP stack from giving memory back when the system really needs it, so you get zombie connections, GUI freezes, or full OOM killer storms instead of graceful degradation.
#net.ipv4.tcp_mem = 1 262144 2147483647 # A
#net.ipv4.tcp_rmem = 1 262144 2147483647 # A
#net.ipv4.tcp_wmem = 1 262144 2147483647 # A
#net.ipv4.udp_mem = 1 262144 2147483647 # A
#net.ipv4.udp_rmem_min = 1 # A
#net.ipv4.udp_wmem_min = 1 # A

# vm - Virtual Memory
#vm.compact_memory = 1 # THIS IS A TRIGGER, NOT A TOGGLE
vm.compact_unevictable_allowed = 1
vm.compaction_proactiveness = 1 #  This setting's background work can consume CPU resources and may cause temporary latency spikes - 0<x<6 is good. Do not use 0. 1 allows, yet dramatically reduces
#vm.drop_caches = 1 # THIS IS A TRIGGER, NOT A TOGGLE
vm.enable_soft_online = 1
vm.extfrag_threshold = 950
vm.max_map_count = 2147483647 # Required for very large LLM processes (e.g. 70B+ models in vLLM/ollama)
vm.highmem_is_dirtyable = 1
vm.mem_profiling = 0 # No stats overhead
vm.memory_failure_recovery = 1
vm.nr_hugepages = 0
vm.nr_trim_pages = 1
vm.hugetlb_optimize_vmemmap = 1
vm.numa_stat = 0
vm.oom_kill_allocating_task = 1 # Kill the offending task immediately instead of random processes — much better for gaming + inference
vm.oom_dump_tasks = 0
vm.overcommit_memory = 1 # Allows large GPU + LLM allocations without instant rejection; safe with zswap+disk backing
vm.panic_on_oom = 0
vm.page-cluster = 0 # minimizes latency, which is more critical for a RAM-based swap device than sequential throughput.
vm.stat_interval = 2000000 # Less stats overhead
vm.swappiness = 10 # Lower value maintains contents in RAM. Despite there being the fast compressed RAM pool, this does NOT control that; 10 maintains performance by maintaining data in RAM. That pool is controlled automatically
vm.vfs_cache_pressure = 50 # Retain filesystem/dentry cache longer → faster IO operations
vm.laptop_mode = 0 # Power-saving feature — disable on desktop for lower I/O latency - TLP enables this if on a laptop, it runs after basic.target
vm.zone_reclaim_mode = 0 # Disable — can cause stalls under memory pressure on NUMA systems; modern kernels handle reclaim better globally
vm.watermark_boost_factor = 0

# https://linuxblog.io/zswap-better-than-zram/
# https://www.youtube.com/watch?v=RGVt16xiERc&t=394 # Reference at the timestamp within the link

# zswap ( Best ) :
# Zswap helps reduce SSD wear compared to traditional disk-based swap
# Prevents OOMs as compared to zram, when you hit max compressible limit
# zswap is generally considered to be handled more intelligently than zram
# dynamic "smart" cache for a disk-based swap device, making more nuanced decisions about memory management
# Disk is indeed somewhat slower(not much for modern NVME SSDs), but its advantages outweight this
# More efficient - handles incompressible data as required
# Lower CPU overhead is better for gaming
# Best overall for modern systems
# Requires you to manually make a swapfile / swap partition ( prefered, more direct, reduces sector access overhead, even on NVMes though negligible advantage )
# Requires you to edit kernel arguments "zswap.enabled = 1". I've already added it to rpm-ostree kargs

# Do not use ZSWAP with ZRAM - it adds unnecessary, uncoordinated overhead between the two
# Keep an on-disk swap partition. I don't care about the priority of it but it needs to be there
# Prevents RAM starvation. This fallback efficiently keeps your unsaved work safe from potential crashes
# This does not back up your unsaved work, just prevents system OOMs that may lead to loss of work
# Likewise, enable Memory Extension on supported phones, prefer the largest or automatic sizing

# zram ( Only on memory constrained devices ) :
# Theoretically lowest latency
# Adds CPU strain, high temps even at idle especially if a memory hogging browser is running in the background
# Blind compression of data - the process becomes less efficient and consumes a significant number of CPU cycles as the compression algorithm attempts to find non-existent patterns
# Usually only better for systems with memory lower than 4GB
# Needs a good enough CPU for handling its strain fast
# No hassle of manually making swap devices - systemd's zram generator does it all deriving it from the configuration

# The following are non functionable, they are just for reference
#zswap.enabled = 1
#zswap.zpool = zsmalloc ## best combination of compression and performance - default on most systems
#zswap.shrinker_enabled = 1
#zswap.compressor = lz4 ## faster, even though consumes more storage, worth it
#zswap.same_filled_pages_enabled = 1
#zswap.non_same_filled_pages_enabled = 1
#zswap.max_pool_percent = 35 ## 100% is unnecessary and harmful ZSWAP should be small to prevent runaway compression layers.

# 'A', Automatic means that the kernel handles this by itself at startup calculation or adjustment during runtime and there is practically no need to adjust the parameter
# What values does the kernel automatically calculate on boot/adjusts dynamically and does NOT use pre-defined values?
# They should be decided by the kernel based on system parameters, NOT be pre-defined constants declared by the developers. I could not find reliable documentation providing the specific information.
# Lines marked as 'A' are commented out to let the kernel decide. The written options in it are for some fallback, having no effect.

# Values for the "default" on a scale will be edited rarely. Only limits for min and max will be usually modified.

# Sources considered in the creation:
# https://n1kobg.blogspot.com/p/blog-page_23.html?m=1

# Filesystem
fs.file-max = 9223372036854775806 # Allows more files to stay open
fs.aio-max-nr = 2147483647
fs.inotify.max_user_instances=2147483647
fs.inotify.max_user_watches=2147483647

# Kernel main parameters
kernel.oops_limit = 0
kernel.panic_on_oops = 0
kernel.panic_on_stackoverflow = 0
kernel.sched_migration_cost_ns = 5000000 # Keeps threads on the same core for longer
kernel.sched_autogroup_enabled = 0 # meant to improve fairness tasks, but hurts performance for HPC and gaming. https://forum.proxmox.com/threads/increase-performance-with-sched_autogroup_enabled-0.41729/ # http://www.youtube.com/watch?v=prxInRdaNfc # 1 for overall desktop efficiency, 0 for datacenter HPC and LLM inferencing
kernel.sched_schedstats = 0 # Less debugging overhead
kernel.sysrq = 1
kernel.split_lock_mitigate=0
# No watchdogs - less debugging overhead
kernel.nmi_watchdog=0
kernel.soft_watchdog=0
kernel.watchdog=0
kernel.pid_max = 4194304 # Large number of allocatable processes

# net - Networking
net.core.bpf_jit_enable = 1 # converts BPF bytecode into native machine code for faster execution. Set to 2 for logs
net.core.bpf_jit_harden = 0 # Hardening costs performance → disable on trusted desktop
net.core.dev_weight = 1024 # A higher dev_weight can allow the kernel to process more packets in a single pass, potentially reducing CPU overhead and improving throughput for high-bandwidth applications. But 65536 can starve userspace, cause GUI stuttering
net.core.dev_weight_tx_bias = 1 # This bias can starve rx performance
net.core.netdev_budget = 2048
net.core.skb_defer_max = 256
net.core.netdev_max_backlog = 32768 # 2147483647 OR 1024, not used
net.core.optmem_max = 262144 # 2147483647, not used
#net.core.rmem_default = 262144 # A
#net.core.rmem_max = 2147483647 # A
net.core.somaxconn = 2147483647
#net.core.wmem_default = 262144 # A
#net.core.wmem_max = 2147483647 # A
net.ipv4.autoconf = 1 # enables automatic configuration of IPv4 interfaces
net.ipv4.addr_gen_mode = 0
net.ipv4.tcp_shrink_window = 1 
net.ipv4.tcp_window_scaling = 1 # allows TCP window sizes to exceed the original 65,535-byte limit
net.ipv4.tcp_max_syn_backlog = 2147483647
net.ipv4.tcp_syncookies = 1 # Protects against attacks
net.ipv4.tcp_tw_reuse = 1 # Allow fast socket reuse
net.ipv4.accept_ra = 2
net.ipv4.tcp_autocorking = 1
net.ipv4.ip_dynaddr = 1 # dynamically change the source IP address of packets and sockets during retransmissions
net.ipv4.tcp_fastopen = 3 # allows clients to send data in the initial SYN packet, potentially reducing latency
net.ipv4.tcp_low_latency = 1 # Latency prioritized over throughput
net.ipv4.tcp_mtu_probing = 2
net.ipv4.tcp_moderate_rcvbuf = 1 # dynamically picks an ideal receive buffer between your min and max based on measured BDP (bandwidth-delay product) and application read rate
#net.ipv4.tcp_max_tw_buckets = 2000000 # A
# Automatic prevents OOM errors and excess starvation. Forcing 2147483647 is almost always worse.
# A single idle TCP socket can still grow huge if some buggy app or peer triggers the auto-tuning logic\
# Setting minimums to 1 makes the system less stable under memory pressure, not more. It prevents the TCP stack from giving memory back when the system really needs it, so you get zombie connections, GUI freezes, or full OOM killer storms instead of graceful degradation.
#net.ipv4.tcp_mem = 1 262144 2147483647 # A
#net.ipv4.tcp_rmem = 1 262144 2147483647 # A
#net.ipv4.tcp_wmem = 1 262144 2147483647 # A
#net.ipv4.udp_mem = 1 262144 2147483647 # A
#net.ipv4.udp_rmem_min = 1 # A
#net.ipv4.udp_wmem_min = 1 # A

# vm - Virtual Memory
#vm.compact_memory = 1 # THIS IS A TRIGGER, NOT A TOGGLE
vm.compact_unevictable_allowed = 1
vm.compaction_proactiveness = 20 # Default 20 → raised slightly from 1 for more proactive defrag without excessive CPU use under heavy load. smoother huge page allocations.
#vm.drop_caches = 1 # THIS IS A TRIGGER, NOT A TOGGLE
vm.enable_soft_online = 1
vm.extfrag_threshold = 950
vm.max_map_count = 2147483647 # Required for very large LLM processes (e.g. 70B+ models in vLLM/ollama)
vm.highmem_is_dirtyable = 1
vm.mem_profiling = 0
vm.memory_failure_recovery = 1
vm.nr_hugepages = 0
vm.nr_trim_pages = 1
vm.hugetlb_optimize_vmemmap = 1
vm.numa_stat = 0
vm.oom_kill_allocating_task = 1 # Kill the offending task immediately instead of random processes — much better for gaming + inference
vm.oom_dump_tasks = 0
vm.overcommit_memory = 1 # Allows large GPU + LLM allocations without instant rejection; safe with zswap+disk backing
vm.panic_on_oom = 0
vm.page-cluster = 0 # minimizes latency, which is more critical for a RAM-based swap device than sequential throughput.
vm.stat_interval = 2000000 #No extra stats overhead
vm.swappiness = 50 # With zswap only: higher value encourages use of fast compressed RAM pool before hitting raw NVMe; 50 is the sweet spot for gaming+LLM on NVMe
vm.vfs_cache_pressure = 50 # Aggressively retain filesystem/dentry cache → faster game loading, faster model loading from NVMe
vm.laptop_mode = 0 # Power-saving feature — disable on desktop for lower I/O latency - TLP enables this if on a laptop, it runs after basic.target
vm.zone_reclaim_mode = 0 # Disable — can cause stalls under memory pressure on NUMA systems; modern kernels handle reclaim better globally

# https://linuxblog.io/zswap-better-than-zram/
# https://www.youtube.com/watch?v=RGVt16xiERc&t=394 # Reference at the timestamp within the link

# zswap ( Best overall) :
# Zswap helps reduce SSD wear compared to traditional disk-based swap
# Prevents OOMs as compared to zram, when you hit max compressible limit
# zswap is generally considered to be handled more intelligently than zram
# dynamic "smart" cache for a disk-based swap device, making more nuanced decisions about memory management
# Disk is indeed somewhat slower(not much for modern NVME SSDs), but its advantages outweight this
# More efficient - handles incompressible data as required
# Lower CPU overhead is better for gaming
# Best overall for modern systems
# Requires you to manually make a swapfile / swap partition ( prefered, more direct, reduces sector access overhead, even on NVMes though negligible advantage )
# Requires you to edit kernel arguments "zswap.enabled = 1". I've already added it to rpm-ostree kargs

# Avoid ZSWAP with ZRAM - it adds unnecessary, uncoordinated overhead between the two
# And another thing, you NEED to have a spare on-disk swap device. I don't care about the priority of it but it needs to be there
# Prevents RAM starvation. This is not a fallback to a regressive-er hardware, it is a backup plan to keep your unsaved work safe from crashes
# This does not back up your unsaved work, just prevents system OOMs that may lead to loss of work
# Likewise, keep Memory Extension also enabled on supported phones, any size, prefer the largest if you have a lot of free storage

# zram ( Only on memory constrained devices) :
# Theoretically lowest latency
# Adds CPU strain, high temps even at idle especially if a memory hogging browser is running in the background
# Blind compression of data - the process becomes less efficient and consumes a significant number of CPU cycles as the compression algorithm attempts to find non-existent patterns
# Usually only better for systems with memory lower than 4GB
# Needs a good enough CPU for handling its strain fast
# No hassle of manually making swap devices - systemd's zram generator does it all deriving it from the configuration

# The following are non functionable, they are just for reference
#zswap.enabled = 1
#zswap.zpool = zsmalloc ## best combination of compression and performance - default on most systems
#zswap.shrinker_enabled = 1
#zswap.compressor = lz4 ## faster, even though consumes more storage, worth it
#zswap.same_filled_pages_enabled = 1
#zswap.non_same_filled_pages_enabled = 1
#zswap.max_pool_percent = 35 ## 100% is unnecessary and harmful ZSWAP should be small to prevent runaway compression layers.
